### 标量导数

 <img src="../image/矩阵计算/标量导数.png" alt="标量导数" style="zoom:50%;" />

###   亚导数

 <img src="../image/矩阵计算/亚导数.png" alt="亚导数" style="zoom:50%;" />

### 梯度

**形状要搞对**

梯度指向值变化最大的方向  -> 今后所有机器学习求解的核心思想

 <img src="../image/矩阵计算/梯度.png" alt="梯度" style="zoom: 67%;" />

#### 样例

 <img src="../image/矩阵计算/梯度样例.png" alt="梯度样例" style="zoom:50%;" />

dy/dx是行向量，dy/dx是列向量

* 这个被称之为**分子布局符号**，反过来的版本叫分母布局符号

<img src="../image/矩阵计算/向量相除.png" alt="向量相除" style="zoom:50%;" /> 

#### **向量相除样例**

-> 第一行后两个比较重要

 <img src="../image/矩阵计算/向量相除样例.png" alt="向量相除样例" style="zoom:50%;" />

#### **拓展到矩阵**

 <img src="../image/矩阵计算/拓展到矩阵.png" alt="拓展到矩阵" style="zoom: 67%;" />





### 向量链式法则

 <img src="../image/矩阵计算/向量链式法则.png" alt="向量链式法则" style="zoom:50%;" />

#### 样式1

 <img src="../image/矩阵计算/向量链式样例1.png" alt="向量链式样例1" style="zoom:50%;" />

#### 样例2

 <img src="../image/矩阵计算/向量链式样式2.png" alt="向量链式样式2" style="zoom:50%;" />



### 自动求导

 <img src="../image/矩阵计算/自动求导.png" alt="自动求导" style="zoom:50%;" />

#### 计算图

* 将代码分解成操作子
* 将计算表示成一个无环图

 <img src="../image/矩阵计算/计算图.png" alt="计算图" style="zoom:50%;" />

```py
# 显示构造
from mxnet import sym

a = sym.var()
b = sym.var()
c = 2 * a + b
```

```py
# 隐式构造
from mxnet import autograd, nd

with autograd.record():
    a = nd.ones((2, 1))
    b = nd.ones((2, 1))
    c = 2 * a + b
```

### 自动求导的两种模式

 <img src="../image/矩阵计算/自动求导的两种模式.png" alt="自动求导的两种模式" style="zoom:50%;" />

#### 方向累积

 <img src="../image/矩阵计算/反向累计.png" alt="反向累计" style="zoom:50%;" />

反向累积总结



复杂度

* 计算复杂度：O(n), n是操作子个数
  * 通常正向和方向的代价类以
* 内存复杂度：O(n)，因为需要存储正向的所有中间结果
* 跟正向累积对比：
  * O(n) 计算复杂度用来计算一个变量的梯度
  * O(1)内存复杂度
